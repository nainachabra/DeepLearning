{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " sequence to sequence LSTM and GRU models for Language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"hin.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2979\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 90\n",
      "Max sequence length for inputs: 107\n",
      "Max sequence length for outputs: 123\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):Aakash Gangurde\n",
    "\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 14s 94ms/step - loss: 1.2475 - accuracy: 0.7866 - val_loss: 1.5151 - val_accuracy: 0.6875\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.9164 - accuracy: 0.8083 - val_loss: 1.7071 - val_accuracy: 0.6873\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.8279 - accuracy: 0.8095 - val_loss: 1.3365 - val_accuracy: 0.6884\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.7837 - accuracy: 0.8112 - val_loss: 1.2806 - val_accuracy: 0.6885\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.7739 - accuracy: 0.8154 - val_loss: 1.2314 - val_accuracy: 0.6918\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.6962 - accuracy: 0.8266 - val_loss: 1.2103 - val_accuracy: 0.7064\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.6386 - accuracy: 0.8394 - val_loss: 1.0807 - val_accuracy: 0.7234\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.6048 - accuracy: 0.8472 - val_loss: 1.0061 - val_accuracy: 0.7422\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.5660 - accuracy: 0.8536 - val_loss: 0.9826 - val_accuracy: 0.7425\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.5389 - accuracy: 0.8591 - val_loss: 2.6206 - val_accuracy: 0.1705\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.5822 - accuracy: 0.8428 - val_loss: 0.9202 - val_accuracy: 0.7555\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.5086 - accuracy: 0.8662 - val_loss: 0.9251 - val_accuracy: 0.7559\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.4967 - accuracy: 0.8688 - val_loss: 0.8799 - val_accuracy: 0.7669\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4839 - accuracy: 0.8715 - val_loss: 0.8572 - val_accuracy: 0.7719\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.4729 - accuracy: 0.8737 - val_loss: 0.8679 - val_accuracy: 0.7663\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4633 - accuracy: 0.8751 - val_loss: 0.8534 - val_accuracy: 0.7708\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.4535 - accuracy: 0.8772 - val_loss: 0.8371 - val_accuracy: 0.7769\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4463 - accuracy: 0.8783 - val_loss: 0.8219 - val_accuracy: 0.7761\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4381 - accuracy: 0.8795 - val_loss: 0.8427 - val_accuracy: 0.7711\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4314 - accuracy: 0.8813 - val_loss: 0.8012 - val_accuracy: 0.7811\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4246 - accuracy: 0.8823 - val_loss: 0.7941 - val_accuracy: 0.7820\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.4178 - accuracy: 0.8838 - val_loss: 0.7958 - val_accuracy: 0.7825\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4090 - accuracy: 0.8855 - val_loss: 0.7774 - val_accuracy: 0.7876\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.4030 - accuracy: 0.8873 - val_loss: 0.7806 - val_accuracy: 0.7865\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3969 - accuracy: 0.8887 - val_loss: 0.7630 - val_accuracy: 0.7920\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3916 - accuracy: 0.8902 - val_loss: 0.7804 - val_accuracy: 0.7879\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.3859 - accuracy: 0.8916 - val_loss: 0.7781 - val_accuracy: 0.7855\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3817 - accuracy: 0.8928 - val_loss: 0.7550 - val_accuracy: 0.7932\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.3757 - accuracy: 0.8941 - val_loss: 0.7574 - val_accuracy: 0.7934\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3688 - accuracy: 0.8961 - val_loss: 0.7568 - val_accuracy: 0.7913\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.3634 - accuracy: 0.8977 - val_loss: 0.7585 - val_accuracy: 0.7927\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3571 - accuracy: 0.8993 - val_loss: 0.7644 - val_accuracy: 0.7924\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.3513 - accuracy: 0.9010 - val_loss: 0.7607 - val_accuracy: 0.7934\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3462 - accuracy: 0.9024 - val_loss: 0.7547 - val_accuracy: 0.7962\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.3418 - accuracy: 0.9036 - val_loss: 0.7662 - val_accuracy: 0.7927\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.3377 - accuracy: 0.9047 - val_loss: 0.7552 - val_accuracy: 0.7955\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.3321 - accuracy: 0.9059 - val_loss: 0.7389 - val_accuracy: 0.7992\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.3262 - accuracy: 0.9077 - val_loss: 0.7798 - val_accuracy: 0.7926\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.3231 - accuracy: 0.9087 - val_loss: 0.7696 - val_accuracy: 0.7935\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.3186 - accuracy: 0.9100 - val_loss: 0.7582 - val_accuracy: 0.7951\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.3197 - accuracy: 0.9098 - val_loss: 0.7570 - val_accuracy: 0.7969\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.3104 - accuracy: 0.9119 - val_loss: 0.7506 - val_accuracy: 0.7979\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.3042 - accuracy: 0.9140 - val_loss: 0.7688 - val_accuracy: 0.7943\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.2997 - accuracy: 0.9148 - val_loss: 0.7659 - val_accuracy: 0.7963\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.2940 - accuracy: 0.9167 - val_loss: 0.7651 - val_accuracy: 0.7965\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.2893 - accuracy: 0.9179 - val_loss: 0.7859 - val_accuracy: 0.7944\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2850 - accuracy: 0.9190 - val_loss: 0.7934 - val_accuracy: 0.7925\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.2806 - accuracy: 0.9203 - val_loss: 0.7868 - val_accuracy: 0.7947\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.2754 - accuracy: 0.9221 - val_loss: 0.7960 - val_accuracy: 0.7932\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2735 - accuracy: 0.9226 - val_loss: 0.7948 - val_accuracy: 0.7924\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2653 - accuracy: 0.9249 - val_loss: 0.8039 - val_accuracy: 0.7919\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2632 - accuracy: 0.9253 - val_loss: 0.7929 - val_accuracy: 0.7942\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2586 - accuracy: 0.9267 - val_loss: 0.8173 - val_accuracy: 0.7924\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2538 - accuracy: 0.9285 - val_loss: 0.8094 - val_accuracy: 0.7937\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.2490 - accuracy: 0.9296 - val_loss: 0.8146 - val_accuracy: 0.7927\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2451 - accuracy: 0.9309 - val_loss: 0.8363 - val_accuracy: 0.7882\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2405 - accuracy: 0.9321 - val_loss: 0.8359 - val_accuracy: 0.7904\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2389 - accuracy: 0.9329 - val_loss: 0.8295 - val_accuracy: 0.7908\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2295 - accuracy: 0.9363 - val_loss: 0.8439 - val_accuracy: 0.7902\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.2285 - accuracy: 0.9360 - val_loss: 0.8609 - val_accuracy: 0.7898\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2246 - accuracy: 0.9372 - val_loss: 0.8710 - val_accuracy: 0.7878\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2207 - accuracy: 0.9384 - val_loss: 0.8587 - val_accuracy: 0.7901\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2173 - accuracy: 0.9394 - val_loss: 0.8907 - val_accuracy: 0.7889\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2144 - accuracy: 0.9405 - val_loss: 0.8856 - val_accuracy: 0.7881\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2099 - accuracy: 0.9414 - val_loss: 0.8872 - val_accuracy: 0.7889\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2060 - accuracy: 0.9428 - val_loss: 0.9375 - val_accuracy: 0.7846\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2032 - accuracy: 0.9439 - val_loss: 0.9003 - val_accuracy: 0.7867\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1993 - accuracy: 0.9451 - val_loss: 0.9234 - val_accuracy: 0.7854\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1950 - accuracy: 0.9464 - val_loss: 0.9183 - val_accuracy: 0.7886\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1992 - accuracy: 0.9449 - val_loss: 0.9184 - val_accuracy: 0.7873\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1832 - accuracy: 0.9506 - val_loss: 0.9470 - val_accuracy: 0.7849\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1853 - accuracy: 0.9495 - val_loss: 0.9363 - val_accuracy: 0.7865\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1851 - accuracy: 0.9490 - val_loss: 0.9499 - val_accuracy: 0.7851\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1820 - accuracy: 0.9502 - val_loss: 0.9644 - val_accuracy: 0.7849\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1789 - accuracy: 0.9509 - val_loss: 0.9607 - val_accuracy: 0.7854\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1766 - accuracy: 0.9519 - val_loss: 0.9839 - val_accuracy: 0.7824\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.1732 - accuracy: 0.9525 - val_loss: 0.9956 - val_accuracy: 0.7820\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1712 - accuracy: 0.9532 - val_loss: 0.9896 - val_accuracy: 0.7842\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1689 - accuracy: 0.9541 - val_loss: 0.9883 - val_accuracy: 0.7847\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1659 - accuracy: 0.9553 - val_loss: 0.9998 - val_accuracy: 0.7837\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1638 - accuracy: 0.9559 - val_loss: 1.0178 - val_accuracy: 0.7843\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1604 - accuracy: 0.9569 - val_loss: 1.0158 - val_accuracy: 0.7833\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1584 - accuracy: 0.9572 - val_loss: 1.0383 - val_accuracy: 0.7827\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1558 - accuracy: 0.9579 - val_loss: 1.0297 - val_accuracy: 0.7819\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1541 - accuracy: 0.9583 - val_loss: 1.0473 - val_accuracy: 0.7843\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1515 - accuracy: 0.9590 - val_loss: 1.0518 - val_accuracy: 0.7830\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1522 - accuracy: 0.9591 - val_loss: 1.0583 - val_accuracy: 0.7826\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1514 - accuracy: 0.9591 - val_loss: 1.0567 - val_accuracy: 0.7826\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1445 - accuracy: 0.9611 - val_loss: 1.0572 - val_accuracy: 0.7828\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1428 - accuracy: 0.9618 - val_loss: 1.0583 - val_accuracy: 0.7843\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1406 - accuracy: 0.9622 - val_loss: 1.0879 - val_accuracy: 0.7824\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1386 - accuracy: 0.9628 - val_loss: 1.0893 - val_accuracy: 0.7830\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1369 - accuracy: 0.9636 - val_loss: 1.1001 - val_accuracy: 0.7813\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1355 - accuracy: 0.9636 - val_loss: 1.0995 - val_accuracy: 0.7816\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1355 - accuracy: 0.9636 - val_loss: 1.0963 - val_accuracy: 0.7833\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1313 - accuracy: 0.9648 - val_loss: 1.1289 - val_accuracy: 0.7819\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1294 - accuracy: 0.9655 - val_loss: 1.1314 - val_accuracy: 0.7823\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.1277 - accuracy: 0.9659 - val_loss: 1.1367 - val_accuracy: 0.7818\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1266 - accuracy: 0.9665 - val_loss: 1.1431 - val_accuracy: 0.7796\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1242 - accuracy: 0.9669 - val_loss: 1.1635 - val_accuracy: 0.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000022C3A2768B0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000022C504833A0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(\"s2s\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: क्या आपको अपना नाप पता है?\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: तुम किस बाल कर सके नहर क्या हि?\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: तुम किस बाल कर सके नहर क्या हि?\n",
      "\n",
      "-\n",
      "Input sentence: Exhale.\n",
      "Decoded sentence: मुझे उसकी पता जानता है।\n",
      "\n",
      "-\n",
      "Input sentence: Exhale.\n",
      "Decoded sentence: मुझे उसकी पता जानता है।\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: वह कमीशा दे हिए।\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: मुझे उसकी पता जानता है।\n",
      "\n",
      "-\n",
      "Input sentence: Inhale.\n",
      "Decoded sentence: वह कमीशा दे हिए।\n",
      "\n",
      "-\n",
      "Input sentence: Inhale.\n",
      "Decoded sentence: वह कमीशा दे हिए।\n",
      "\n",
      "-\n",
      "Input sentence: Thanks!\n",
      "Decoded sentence: वह कमरे में माब करतां।\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: हम सब तुम्हारी सेहत के लिए बहुत परेशान हैं।\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: वह काम शुर माल करता है।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2979\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 90\n",
      "Max sequence length for inputs: 107\n",
      "Max sequence length for outputs: 123\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 8s 73ms/step - loss: 1.3428 - accuracy: 0.7861 - val_loss: 1.8868 - val_accuracy: 0.6876\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9046 - accuracy: 0.8084 - val_loss: 1.6402 - val_accuracy: 0.6873\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.7907 - accuracy: 0.8099 - val_loss: 1.2800 - val_accuracy: 0.6901\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.7349 - accuracy: 0.8177 - val_loss: 1.1734 - val_accuracy: 0.7006\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.6572 - accuracy: 0.8343 - val_loss: 1.0859 - val_accuracy: 0.7367\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.6126 - accuracy: 0.8469 - val_loss: 1.0170 - val_accuracy: 0.7333\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.5632 - accuracy: 0.8528 - val_loss: 0.9483 - val_accuracy: 0.7487\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.5322 - accuracy: 0.8596 - val_loss: 0.9322 - val_accuracy: 0.7542\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.5121 - accuracy: 0.8659 - val_loss: 0.8909 - val_accuracy: 0.7632\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.4929 - accuracy: 0.8691 - val_loss: 0.8585 - val_accuracy: 0.7698\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.4786 - accuracy: 0.8716 - val_loss: 0.8480 - val_accuracy: 0.7731\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.4669 - accuracy: 0.8743 - val_loss: 0.8381 - val_accuracy: 0.7750\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.4562 - accuracy: 0.8764 - val_loss: 0.8262 - val_accuracy: 0.7780\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.4470 - accuracy: 0.8782 - val_loss: 0.8221 - val_accuracy: 0.7764\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.4387 - accuracy: 0.8790 - val_loss: 0.8018 - val_accuracy: 0.7792\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.4309 - accuracy: 0.8805 - val_loss: 0.8025 - val_accuracy: 0.7784\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.4237 - accuracy: 0.8823 - val_loss: 0.7814 - val_accuracy: 0.7841\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.4171 - accuracy: 0.8835 - val_loss: 0.7788 - val_accuracy: 0.7842\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.4107 - accuracy: 0.8849 - val_loss: 0.7721 - val_accuracy: 0.7873\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.4052 - accuracy: 0.8861 - val_loss: 0.7675 - val_accuracy: 0.7873\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.3991 - accuracy: 0.8875 - val_loss: 0.7591 - val_accuracy: 0.7889\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.3936 - accuracy: 0.8891 - val_loss: 0.7455 - val_accuracy: 0.7940\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3887 - accuracy: 0.8902 - val_loss: 0.7474 - val_accuracy: 0.7925\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3839 - accuracy: 0.8913 - val_loss: 0.7432 - val_accuracy: 0.7950\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.3787 - accuracy: 0.8929 - val_loss: 0.7351 - val_accuracy: 0.7962\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3740 - accuracy: 0.8939 - val_loss: 0.7440 - val_accuracy: 0.7949\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3698 - accuracy: 0.8953 - val_loss: 0.7263 - val_accuracy: 0.7996\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.3651 - accuracy: 0.8963 - val_loss: 0.7198 - val_accuracy: 0.8012\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 0.3606 - accuracy: 0.8974 - val_loss: 0.7187 - val_accuracy: 0.8016\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3563 - accuracy: 0.8986 - val_loss: 0.7207 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.3522 - accuracy: 0.8999 - val_loss: 0.7126 - val_accuracy: 0.8036\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.3479 - accuracy: 0.9006 - val_loss: 0.7070 - val_accuracy: 0.8046\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3439 - accuracy: 0.9020 - val_loss: 0.7207 - val_accuracy: 0.8032\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3393 - accuracy: 0.9035 - val_loss: 0.7209 - val_accuracy: 0.8027\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3356 - accuracy: 0.9042 - val_loss: 0.7131 - val_accuracy: 0.8055\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3315 - accuracy: 0.9055 - val_loss: 0.7109 - val_accuracy: 0.8050\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.3273 - accuracy: 0.9067 - val_loss: 0.7128 - val_accuracy: 0.8059\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.3231 - accuracy: 0.9078 - val_loss: 0.7141 - val_accuracy: 0.8063\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 0.3192 - accuracy: 0.9086 - val_loss: 0.7176 - val_accuracy: 0.8050\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3151 - accuracy: 0.9100 - val_loss: 0.7119 - val_accuracy: 0.8062\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3110 - accuracy: 0.9107 - val_loss: 0.7214 - val_accuracy: 0.8061\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3070 - accuracy: 0.9122 - val_loss: 0.7086 - val_accuracy: 0.8084\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.3031 - accuracy: 0.9134 - val_loss: 0.7070 - val_accuracy: 0.8080\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2988 - accuracy: 0.9145 - val_loss: 0.7281 - val_accuracy: 0.8049\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2949 - accuracy: 0.9155 - val_loss: 0.7161 - val_accuracy: 0.8073\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.2906 - accuracy: 0.9169 - val_loss: 0.7339 - val_accuracy: 0.8049\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2866 - accuracy: 0.9177 - val_loss: 0.7284 - val_accuracy: 0.8060\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2826 - accuracy: 0.9189 - val_loss: 0.7330 - val_accuracy: 0.8063\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2783 - accuracy: 0.9205 - val_loss: 0.7250 - val_accuracy: 0.8074\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.2748 - accuracy: 0.9211 - val_loss: 0.7388 - val_accuracy: 0.8073\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2699 - accuracy: 0.9226 - val_loss: 0.7249 - val_accuracy: 0.8069\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.2662 - accuracy: 0.9240 - val_loss: 0.7357 - val_accuracy: 0.8061\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2619 - accuracy: 0.9249 - val_loss: 0.7474 - val_accuracy: 0.8051\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2580 - accuracy: 0.9261 - val_loss: 0.7523 - val_accuracy: 0.8056\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.2541 - accuracy: 0.9276 - val_loss: 0.7512 - val_accuracy: 0.8052\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.7507 - val_accuracy: 0.8060\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.2465 - accuracy: 0.9294 - val_loss: 0.7585 - val_accuracy: 0.8054\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2416 - accuracy: 0.9312 - val_loss: 0.7785 - val_accuracy: 0.8033\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.2383 - accuracy: 0.9319 - val_loss: 0.7796 - val_accuracy: 0.8032\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2340 - accuracy: 0.9337 - val_loss: 0.7842 - val_accuracy: 0.8022\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2306 - accuracy: 0.9345 - val_loss: 0.7858 - val_accuracy: 0.8028\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2263 - accuracy: 0.9360 - val_loss: 0.8012 - val_accuracy: 0.8012\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2232 - accuracy: 0.9363 - val_loss: 0.8074 - val_accuracy: 0.8011\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2188 - accuracy: 0.9379 - val_loss: 0.7984 - val_accuracy: 0.8042\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2152 - accuracy: 0.9390 - val_loss: 0.7999 - val_accuracy: 0.8033\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.2114 - accuracy: 0.9403 - val_loss: 0.8245 - val_accuracy: 0.8006\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.2082 - accuracy: 0.9415 - val_loss: 0.8232 - val_accuracy: 0.8022\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2048 - accuracy: 0.9421 - val_loss: 0.8280 - val_accuracy: 0.8015\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.2011 - accuracy: 0.9434 - val_loss: 0.8366 - val_accuracy: 0.8005\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.1980 - accuracy: 0.9443 - val_loss: 0.8363 - val_accuracy: 0.8009\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.1943 - accuracy: 0.9455 - val_loss: 0.8518 - val_accuracy: 0.8003\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1911 - accuracy: 0.9466 - val_loss: 0.8606 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1880 - accuracy: 0.9474 - val_loss: 0.8518 - val_accuracy: 0.8022\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.1852 - accuracy: 0.9483 - val_loss: 0.8664 - val_accuracy: 0.8004\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1815 - accuracy: 0.9495 - val_loss: 0.8757 - val_accuracy: 0.7974\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.1792 - accuracy: 0.9502 - val_loss: 0.8886 - val_accuracy: 0.7986\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1760 - accuracy: 0.9512 - val_loss: 0.8890 - val_accuracy: 0.7978\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1729 - accuracy: 0.9520 - val_loss: 0.8948 - val_accuracy: 0.7980\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1700 - accuracy: 0.9529 - val_loss: 0.9071 - val_accuracy: 0.7962\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1682 - accuracy: 0.9534 - val_loss: 0.9076 - val_accuracy: 0.7984\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1645 - accuracy: 0.9549 - val_loss: 0.9230 - val_accuracy: 0.7955\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.1624 - accuracy: 0.9553 - val_loss: 0.9321 - val_accuracy: 0.7974\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.1600 - accuracy: 0.9558 - val_loss: 0.9524 - val_accuracy: 0.7933\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1571 - accuracy: 0.9569 - val_loss: 0.9621 - val_accuracy: 0.7942\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1553 - accuracy: 0.9575 - val_loss: 0.9601 - val_accuracy: 0.7933\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1527 - accuracy: 0.9585 - val_loss: 0.9486 - val_accuracy: 0.7962\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.1506 - accuracy: 0.9589 - val_loss: 0.9610 - val_accuracy: 0.7952\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.1487 - accuracy: 0.9594 - val_loss: 0.9492 - val_accuracy: 0.7970\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1457 - accuracy: 0.9604 - val_loss: 0.9690 - val_accuracy: 0.7952\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1446 - accuracy: 0.9606 - val_loss: 0.9823 - val_accuracy: 0.7949\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 2s 46ms/step - loss: 0.1420 - accuracy: 0.9614 - val_loss: 1.0019 - val_accuracy: 0.7950\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.1406 - accuracy: 0.9620 - val_loss: 0.9901 - val_accuracy: 0.7948\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1381 - accuracy: 0.9626 - val_loss: 1.0019 - val_accuracy: 0.7952\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1363 - accuracy: 0.9633 - val_loss: 1.0232 - val_accuracy: 0.7933\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.1347 - accuracy: 0.9637 - val_loss: 1.0226 - val_accuracy: 0.7952\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.1330 - accuracy: 0.9639 - val_loss: 1.0252 - val_accuracy: 0.7943\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.1311 - accuracy: 0.9646 - val_loss: 1.0296 - val_accuracy: 0.7933\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.1297 - accuracy: 0.9649 - val_loss: 1.0444 - val_accuracy: 0.7933\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.1278 - accuracy: 0.9656 - val_loss: 1.0734 - val_accuracy: 0.7913\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 2s 45ms/step - loss: 0.1262 - accuracy: 0.9662 - val_loss: 1.0473 - val_accuracy: 0.7934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000022D0C955A00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000022D09E1DB20> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "data_path = \"hin.txt\"  # Path to the data txt file on disk.\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()Aakash Gangurde\n",
    "\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.GRU(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the state.\n",
    "encoder_states = [state_h]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_gru = keras.layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training the modelAakash Gangurde\n",
    "\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"s2s\")\n",
    "\n",
    "# Define sampling models for inference\n",
    "model = keras.models.load_model(\"s2s\")\n",
    "\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_outputs, state_h_enc = model.layers[2].output\n",
    "encoder_states = [state_h_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h]\n",
    "decoder_gru = model.layers[3]\n",
    "decoder_outputs, state_h_dec = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Exhale.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Exhale.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Inhale.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Inhale.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Thanks!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: मैं उसका दिलना चाहिए थी।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value])  # Ensure states_value is in a list\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        states_value = [h]  # Put h in a list\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Testing the model on some input sequences\n",
    "for seq_index in range(20):\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
